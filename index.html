<!DOCTYPE html>
<html lang="en-US">

<head>
    <title>学术个人空间</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Personal Page of ChenYaofo">

    <link rel="canonical" href="" />

    <link rel="icon" media="(prefers-color-scheme:dark)" href="assets/img/favicon-dark.png" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="assets/img/favicon.png" type="image/png" />
    <script src="assets/js/favicon-switcher.js" type="application/javascript"></script>

    <link rel=stylesheet href="assets/css/academicons.min.css">
    <link rel=stylesheet href="assets/css/all.min.css">

    <link rel="stylesheet" href="assets/css/style.css">
</head>

<header class="site-header">

    <div class="wrapper">

        <nav class="site-nav">
            <input type="checkbox" id="nav-trigger" class="nav-trigger" />
            <label for="nav-trigger">
                <span class="menu-icon">
                    <svg viewBox="0 0 18 15" width="18px" height="15px">
                        <path fill="#424242"
                            d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" />
                        <path fill="#424242"
                            d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" />
                        <path fill="#424242"
                            d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" />
                    </svg>
                </span>
            </label>

            <div class="trigger">
                <a class="page-link nav-word" href="#about-me">About Me</a>
                <a class="page-link nav-word" href="#education">Education&Research</a>
                <a class="page-link nav-word" href="#publications">Publications</a>
                <!-- <a class="page-link nav-word" href="#open-source">Open Source</a> -->
                <a class="page-link nav-word" href="#">CV</a> (<a class="nav-word" href="cv-en.pdf">en</a>|<a
                    class="nav-word" href="cv-zh.pdf">zh</a>)
            </div>

        </nav>

    </div>

</header>

<body>
    <div class="wrapper">
        <header>
            <a class="image avatar"><img src="assets/img/avatar.jpg" alt="avatar" onContextMenu="return false" /></a>
            <h1>Yaofo Chen</h1>
            Ph.D. Student
            <br> South China University of Technology
            <br> chenyaofo@gmail.com
            <br>
            <br>

        </header>
        <section>

            <h2 id="about-me">About Me</h2>

            <p>
                I am a Ph.D. candidate supervised by Prof.<a href="https://tanmingkui.github.io/" target="_blank">Mingkui
                    Tan</a> with the School of Software Engineering at <a href="https://www.scut.edu.cn/"
                    target="_blank">South China University of Technology</a>.
                I also received my bachelor degree in Software Engineering from the same university in 2018.
                My research interests include <em>nueral architecture search (NAS)</em> for efficient deep architectures and
                <em>test-time adaptation (TTA)</em> for out-of-distribution
                generalization.
                I published papers in top venues, including ICML, ICLR, CVPR, IEEE TCSVT and Neural Networks.
                I served as the reviewer for several top-tier machine learning/computer vision conferences, including
                ICLR, ICML, NeurIPS, CVPR, ICCV, ECCV and AAAI.
                <br>
            </p>

            <h2 id="education">Education & Research</h2>

            <ul>
                <!-- <li>
                    <p>
                        <strong>BSc in Software Engineering, 2014 ~ 2018</strong>
                        <br />
                        <em>South China University of Technology (SCUT)</em>
                    </p>
                </li>
                <li>
                    <p>
                        <strong>MSc in Software Engineering, 2018 ~ 2020</strong>
                        <br />
                        <em>South China University of Technology (SCUT)</em>
                    </p>
                </li> -->
                <li>
                    <p>
                        <strong>Ph.D. candidate in Software Engineering, 2020 ~ 2024 (Expected)</strong>
                        <br />
                        <em><a href="https://www.scut.edu.cn/" target="_blank">South China University of
                                Technology</a>, Guangzhou, China</em>
                    </p>
                </li>
                <li>
                    <p>
                        <strong>Research Intern, since Sep. 2021, ongoing</strong>
                        <br />
                        <em><a href="https://www.pcl.ac.cn/" target="_blank">Peng Cheng Laboratory</a>, Shenzhen,
                            China</em>
                    </p>
                </li>
            </ul>

            <h2 id="publications">Publications</h2>

            <ul>
                <li>
                    <p>
                        <strong>
                            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Contrastive_Neural_Architecture_Search_With_Neural_Architecture_Comparators_CVPR_2021_paper.html"
                                target="_blank">
                                Contrastive Neural Architecture Search with Neural Architecture Comparators
                            </a>
                        </strong>
                        <br />
                        <u><em><strong>Yaofo Chen</strong></em></u>, Yong Guo, Qi Chen, Minli Li, Wei Zeng, Yaowei Wang,
                        Mingkui Tan
                        <br /> <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021)</strong>
                        <br /> [
                        <a href="papers/chen-contrastive-neural-architecture-search-with-neural-architecture-comparators.pdf"
                            target="_blank">PDF</a>] [
                        <a href="papers/chen-contrastive-neural-architecture-search-with-neural-architecture-comparators-supp.pdf"
                            target="_blank">Supp</a>] [
                        <a href="posters/chen-contrastive-neural-architecture-search-with-neural-architecture-comparators-poster.pdf"
                            target="_blank">Poster</a>] [
                        <a href="slides/chen-contrastive-neural-architecture-search-with-neural-architecture-comparators-slides.pdf"
                            target="_blank">Slides</a>] [
                        <a href="https://github.com/chenyaofo/CTNAS" target="_blank">Code</a>] [
                        <a href="bibtex/chen2021contrastive.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://openreview.net/forum?id=vePdNU3u6n" target="_blank">
                                Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation
                            </a>
                        </strong>
                        <br /> <u><em><strong>Yaofo Chen</strong></em></u>, Shuaicheng Niu, Yaowei Wang, Shoukai Xu, Hengjie Song, Mingkui Tan
                        <br />
                        <strong>International Conference on Learning Representations (ICLR 2024)</strong>
                        <br />
                        [<a href="papers/chen-towards-robust-and-efficient-cloud-edge-elastic-model-adaptation-via-selective-entropy-distillation.pdf"
                            target="_blank">PDF</a>]
                        [<a href="posters/chen-towards-robust-and-efficient-cloud-edge-elastic-model-adaptation-via-selective-entropy-distillation-poster.pdf"
                            target="_blank">Poster</a>]
                        [<a href="slides/chen-towards-robust-and-efficient-cloud-edge-elastic-model-adaptation-via-selective-entropy-distillation-slides.pdf"
                            target="_blank">Slides</a>]
                        [<a href="https://github.com/chenyaofo/CEMA" target="_blank">Code</a>]
                        [<a href="bibtex/chen2022towards.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://doi.org/10.1109/TCSVT.2024.3395463" target="_blank">
                                Automated Dominative Subspace Mining for Efficient Neural Architecture Search
                            </a>
                        </strong>
                        <br /> <u><em><strong>Yaofo Chen</strong></em></u>, Yong Guo, Daihai Liao, Fanbing Lv, Hengjie
                        Song, James Tin-Yau Kwok, Mingkui Tan
                        <br />
                        <strong>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT 2024)</strong>
                        <br /> [
                        <a href="papers/chen-automated-dominative-subspace-mining-for-efficient-neural-architecture-search.pdf"
                            target="_blank">PDF</a>] [
                        <a href="https://github.com/chenyaofo/ASE-NAS" target="_blank">Code</a>] [
                        <a href="bibtex/chen2024automatic.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://proceedings.mlr.press/v119/guo20b.html" target="_blank">
                                Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Search
                            </a>
                        </strong>
                        <br /> <u><em><strong>Yaofo Chen<sup>*</sup></strong></em></u>, Yong Guo<sup>*</sup>,
                        Yin Zheng, Peilin Zhao, Jian Chen, Junzhou Huang, Mingkui Tan
                        <br /> <strong>Proceedings of the International Conference on Machine Learning (ICML
                            2020)</strong>
                        <br /> [
                        <a href="papers/guo-breaking-the-curse-of-space-explosion-towards-efficient-NAS-with-curriculum-search.pdf"
                            target="_blank">PDF</a>] [
                        <a href="papers/guo-breaking-the-curse-of-space-explosion-towards-efficient-NAS-with-curriculum-search-supp.pdf"
                            target="_blank">Supp</a>] [
                        <a href="posters/guo-breaking-the-curse-of-space-explosion-towards-efficient-NAS-with-curriculum-search-poster.pdf"
                            target="_blank">Poster</a>] [
                        <a href="slides/guo-breaking-the-curse-of-space-explosion-towards-efficient-NAS-with-curriculum-search-slides.pdf"
                            target="_blank">Slides</a>] [
                        <a href="https://github.com/guoyongcs/CNAS" target="_blank">Code</a>] [
                        <a href="bibtex/guo2020breaking.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="#" target="_blank">
                                Towards Long Video Understanding via Fine-detailed Video Story Generation
                            </a>
                        </strong>
                        <br /> Zeng You<sup>*</sup>, Zhiquan Wen<sup>*</sup>, <u><em><strong>Yaofo Chen</strong></em></u><sup>*</sup>, Xin Li, Runhao Zeng, Yaowei Wang, Mingkui Tan
                        <br />
                        <strong>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT 2024)</strong>
                        <br /> [
                        <a href="#"
                            target="_blank">PDF(To Be Released)</a>] [
                        <a href="#" target="_blank">Code</a>] [
                        <a href="#" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://doi.org/10.1016/j.neunet.2021.06.030" target="_blank">
                                Content-aware Convolutional Neural Networks
                            </a>
                        </strong>
                        <br /> Yong Guo, <u><em><strong>Yaofo Chen</strong></em></u>, Mingkui Tan, Kui Jia, Jian Chen,
                        Jingdong Wang
                        <br />
                        <strong>Neural Networks (2021)</strong>
                        <br /> [
                        <a href="papers/guo-content-aware-convolutional-neural-networks.pdf" target="_blank">PDF</a>] [
                        <a href="https://github.com/guoyongcs/CAC" target="_blank">Code</a>] [
                        <a href="bibtex/guo2021contentaware.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://openreview.net/forum?id=g2YraF75Tj" target="_blank">
                                Towards Stable Test-time Adaptation in Dynamic Wild World
                            </a>
                        </strong>
                        <br /> Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, <u><em><strong>Yaofo
                                    Chen</strong></em></u>, Peilin Zhao, Mingkui Tan
                        <br />
                        <strong>International Conference on Learning Representations (<u><em>ICLR 2023
                                    oral</em></u>)</strong>
                        <br />
                        [<a href="papers/niu-towards-stable-test-time-adaptation-in-dynamic-wild-world.pdf"
                            target="_blank">PDF</a>]
                        [<a href="posters/niu-towards-stable-test-time-adaptation-in-dynamic-wild-world-poster.pdf"
                            target="_blank">Poster</a>]
                        [<a href="slides/niu-towards-stable-test-time-adaptation-in-dynamic-wild-world-slides.pdf"
                            target="_blank">Slides</a>]
                        [<a href="https://github.com/mr-eggplant/SAR" target="_blank">Code</a>]
                        [<a href="bibtex/niu2022towards.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://proceedings.mlr.press/v162/niu22a.html" target="_blank">
                                Efficient Test-Time Model Adaptation without Forgetting
                            </a>
                        </strong>
                        <br /> Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, <u><em><strong>Yaofo Chen</strong></em></u>,
                        Shijian Zheng, Peilin Zhao, Mingkui Tan
                        <br /> <strong>Proceedings of the International Conference on Machine Learning (ICML
                            2022)</strong>
                        <br /> [
                        <a href="papers/niu-efficient-test-time-model-adaptation-without-forgetting.pdf"
                            target="_blank">PDF</a>] [<a
                            href="posters/niu-efficient-test-time-model-adaptation-without-forgetting-poster.png"
                            target="_blank">Poster</a>] [<a
                            href="slides/niu-efficient-test-time-model-adaptation-without-forgetting-slides.pdf"
                            target="_blank">Slides</a>][
                        <a href="https://github.com/mr-eggplant/EATA" target="_blank">Code</a>] [
                        <a href="bibtex/niu2022efficient.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="http://cjc.ict.ac.cn/qwjs/view.asp?id=5746" target="_blank">
                                Class-Balanced Federated Learning Based on Data Generation
                            </a>
                        </strong>
                        <br /> ZhiPeng Li, Yong Guo, <u><em><strong>Yaofo Chen</strong></em></u>, Yaowei Wang, Wei Zeng,
                        Mingkui Tan
                        <br />
                        <strong>Chinese Journal of Computers (2022)</strong>
                        <br /> [
                        <a href="papers/li-class-balanced-federated-learning-based-on-data-generation.pdf"
                            target="_blank">PDF</a>] [
                        <a href="https://github.com/lizhipengcs/CBFL" target="_blank">Code</a>]
                        <!-- <a href="bibtex/li2023class.txt" target="_blank">BibTeX</a>] -->
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Guo_Pareto-Aware_Neural_Architecture_Generation_for_Diverse_Computational_Budgets_CVPRW_2023_paper.html"
                                target="_blank">
                                Pareto-aware Neural Architecture Generation for Diverse Computational Budgets
                            </a>
                        </strong>
                        <br /> Yong Guo, <u><em><strong>Yaofo Chen</strong></em></u>, Yin Zheng, Qi Chen, Peilin Zhao,
                        Jian Chen, Junzhou Huang, Mingkui Tan
                        <br /> <strong>IEEE Conference on Computer Vision and Pattern Recognition Workshop (CVPRW
                            2023)</strong>
                        <br /> [
                        <a href="papers/guo-pareto-aware-neural-architecture-generation-for-diverse-computational-budgets.pdf"
                            target="_blank">PDF</a>] [
                        <a href="papers/guo-pareto-aware-neural-architecture-generation-for-diverse-computational-budgets-supp.pdf"
                            target="_blank">Supp</a>] [
                        <a href="https://github.com/guoyongcs/PNAG" target="_blank">Code</a>] [
                        <a href="bibtex/guo2023pareto.txt" target="_blank">BibTeX</a>]
                    </p>
                </li>
            </ul>

            <p>
                Collaborators:
                <a href="http://www.guoyongcs.com/" target="_blank">Yong Guo (HUAWEI Inc.)</a>,
                <a href="https://niushuaicheng.cn/" target="_blank">Shuaicheng Niu (NTU)</a>,
                <a href="https://dblp.org/pid/260/0276.html" target="_blank">Shoukai Xu (SCUT)</a>,
                <a href="https://github.com/lizhipengcs" target="_blank">ZhiPeng Li (SCUT)</a>,
                <a href="https://chenqi008.github.io/" target="_blank">Qi Chen (Adelaide
                    University)</a>,
                <a href="https://dblp.org/pid/283/6539.html" target="_blank">Minli Li (Tencent Inc.)</a>,
                <a href="https://scholar.google.com/citations?user=o_DllmIAAAAJ" target="_blank">Yaowei Wang (Pengcheng
                    Lab)</a>,
                <a href="https://jingdongwang2017.github.io/" target="_blank">Jingdong Wang (Baidu Inc.)</a>,
                <a href="https://tanmingkui.github.io/" target="_blank">Mingkui Tan (SCUT)</a>
            </p>

            <br>

            <!-- <h2 id="open-source">Open-source Projects (Selected)</h2>

            <ul>
                <li>
                    <p>
                        <strong>
                            <a href="https://github.com/chenyaofo/image-classification-codebase">
                                chenyaofo/image-classification-codebase
                            </a>
                        </strong> : A codebase for the image classification task implemented by PyTorch, which does not
                        use any high-level deep learning libraries (such as pytorch-lightening or MMClassification) and
                        thus is easy
                        to follow and modified.
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://github.com/chenyaofo/pytorch-cifar-models">
                                chenyaofo/pytorch-cifar-models
                            </a>
                        </strong> : A model zoo consisted of pretrained classified models on CIFAR-10/100 in PyTorch,
                        including vgg, resnet, mobilenetv2, shufflenetv2 and so on.
                    </p>
                </li> -->
            <!-- <li>
                    <p>
                        <strong>
                            <a href="https://github.com/chenyaofo/deep-learning-model-deploy-tutorial">
                                chenyaofo/deep-learning-model-deploy-tutorial
                            </a>
                        </strong> : A tutorial for deep learning model deployment, which contains TensorRT, ONNX, PPLNN and OpenVINO frameworks. It also includes a performance benchmark on varying hardwares with above deployment
                        frameworks.
                    </p>
                </li> -->
            <!-- <li>
                    <p>
                        <strong>
                            <a href="https://github.com/chenyaofo/statistical-learning-method-numpy-impl">
                                chenyaofo/statistical-learning-method-numpy-impl
                            </a>
                        </strong> : Numpy implementations for algorithms in the book <em>Statistical Learning Method (by
                            Li Hang)</em>, which is highly efficient and can be directly runing on Colab without cloning
                        the codes to
                        the local.
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://github.com/chenyaofo/awesome-vision-architecture">
                                chenyaofo/awesome-vision-architecture
                            </a>
                        </strong> : An up-to-date list of progress made in deep learning vision architectures, which
                        includes but not limited to papers (backbone design, loss deisgn, tricks etc), datasets,
                        codebases, frameworks
                        and etc.
                    </p>
                </li>
                <li>
                    <p>
                        <strong>
                            <a href="https://github.com/chenyaofo/awesome-architecture-search">
                                chenyaofo/awesome-architecture-search
                            </a>
                        </strong> : An up-to-date list of progress made in neural architecture search, which includes
                        but not limited to papers, datasets, codebases, frameworks and etc.
                    </p>
                </li> -->
            <!-- </ul> -->

            <!-- <br> -->

        </section>
        <footer>
            <div class="social-icons">

                <a href="https://scholar.google.com/citations?user=NHZCt2EAAAAJ" target="_blank">
                    <i class="ai ai-google-scholar" style="font-size:1.6rem"></i>
                </a>

                <a href="https://github.com/chenyaofo" target="_blank">
                    <i class="fab fa-github"></i>
                </a>

            </div>
            <br>
            <p>
                Last updated: Aug. 2024.
                <br>
                <a href="https://beian.miit.gov.cn/" target="_blank">
                    粤ICP备2022089319号
                </a>
            </p>
        </footer>
    </div>
    <script src="assets/js/scale.fix.js"></script>

</body>

</html>
